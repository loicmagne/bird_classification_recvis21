{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T17:45:08.824103Z",
     "iopub.status.busy": "2021-11-22T17:45:08.823765Z",
     "iopub.status.idle": "2021-11-22T17:45:42.411219Z",
     "shell.execute_reply": "2021-11-22T17:45:42.410343Z",
     "shell.execute_reply.started": "2021-11-22T17:45:08.824018Z"
    },
    "id": "hhUaqNOKnnfC"
   },
   "outputs": [],
   "source": [
    "!pip install -U wandb -q\n",
    "!pip install -U albumentations -q\n",
    "!pip install -U transformers datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T17:45:42.414804Z",
     "iopub.status.busy": "2021-11-22T17:45:42.414262Z",
     "iopub.status.idle": "2021-11-22T17:45:46.657598Z",
     "shell.execute_reply": "2021-11-22T17:45:46.656872Z",
     "shell.execute_reply.started": "2021-11-22T17:45:42.414767Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import zipfile\n",
    "import os\n",
    "import wandb\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T17:45:46.659522Z",
     "iopub.status.busy": "2021-11-22T17:45:46.659268Z",
     "iopub.status.idle": "2021-11-22T17:45:47.406053Z",
     "shell.execute_reply": "2021-11-22T17:45:47.405244Z",
     "shell.execute_reply.started": "2021-11-22T17:45:46.659486Z"
    },
    "id": "rmUUpWilnpXu",
    "outputId": "573cbec1-ec04-4aa4-8fc9-0c4b5e705415"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7fg25BKnnfI"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T19:20:42.150109Z",
     "iopub.status.busy": "2021-11-22T19:20:42.149831Z",
     "iopub.status.idle": "2021-11-22T19:20:42.167031Z",
     "shell.execute_reply": "2021-11-22T19:20:42.166155Z",
     "shell.execute_reply.started": "2021-11-22T19:20:42.150076Z"
    },
    "id": "si0bcDtznnfK"
   },
   "outputs": [],
   "source": [
    "size = 224\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((size, size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_transforms = A.Compose([\n",
    "    A.CoarseDropout(p=0.1),\n",
    "    A.GaussNoise(p=0.2),\n",
    "    A.ElasticTransform(p=0.33),\n",
    "    A.Rotate(),\n",
    "    A.ShiftScaleRotate(p=0.2),\n",
    "    A.RGBShift(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.Resize(32, 32,p=0.1), # Randomly downsample\n",
    "    A.Resize(64, 64,p=0.2), # Randomly downsample\n",
    "    A.Resize(size,size,cv2.INTER_LANCZOS4),\n",
    "    A.Downscale(scale_min=0.1,scale_max=0.25,interpolation=cv2.INTER_LANCZOS4,p=0.3),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T19:20:42.352977Z",
     "iopub.status.busy": "2021-11-22T19:20:42.352739Z",
     "iopub.status.idle": "2021-11-22T19:20:42.367262Z",
     "shell.execute_reply": "2021-11-22T19:20:42.366478Z",
     "shell.execute_reply.started": "2021-11-22T19:20:42.352948Z"
    },
    "id": "RfYHHZocwdpZ"
   },
   "outputs": [],
   "source": [
    "class AugmentedDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image=np.array(image))[\"image\"]\n",
    "        return image, label\n",
    "    \n",
    "def visualize_augmentations(dataset, idx=0, samples=10, cols=5):\n",
    "    dataset = copy.deepcopy(dataset)\n",
    "    dataset.transform = A.Compose([t for t in dataset.transform if not isinstance(t, (A.Normalize, ToTensorV2))])\n",
    "    rows = samples // cols\n",
    "    figure, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(12, 6))\n",
    "    for i in range(samples):\n",
    "        image, _ = dataset[idx]\n",
    "        ax.ravel()[i].imshow(image)\n",
    "        ax.ravel()[i].set_axis_off()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MgjJfAnnnfN"
   },
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T19:20:42.909647Z",
     "iopub.status.busy": "2021-11-22T19:20:42.909365Z",
     "iopub.status.idle": "2021-11-22T19:20:42.918004Z",
     "shell.execute_reply": "2021-11-22T19:20:42.917324Z",
     "shell.execute_reply.started": "2021-11-22T19:20:42.909617Z"
    },
    "id": "uqLYvYpHkvIm"
   },
   "outputs": [],
   "source": [
    "from transformers import BeitForImageClassification, ViTForImageClassification, DeiTForImageClassification\n",
    "nclasses = 20\n",
    "class BEiTNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BEiTNet, self).__init__()\n",
    "        self.beit =  BeitForImageClassification.from_pretrained(f'microsoft/beit-large-patch16-224')\n",
    "        # self.beit =  ViTForImageClassification.from_pretrained(f'google/vit-large-patch16-224')\n",
    "        # self.beit =  DeiTForImageClassification.from_pretrained('facebook/deit-base-distilled-patch16-224')\n",
    "        self.beit.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.beit.classifier.in_features,512),\n",
    "            torch.nn.Dropout(p=0.4),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512,nclasses),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.beit(x).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo6vU41DnnfQ"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T19:20:43.954849Z",
     "iopub.status.busy": "2021-11-22T19:20:43.954173Z",
     "iopub.status.idle": "2021-11-22T19:20:49.443257Z",
     "shell.execute_reply": "2021-11-22T19:20:49.442311Z",
     "shell.execute_reply.started": "2021-11-22T19:20:43.954793Z"
    },
    "id": "Voi-ddQryM53",
    "outputId": "5f7cdecd-67fc-4e40-ae44-45cdcfe62ee8"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Training settings\n",
    "config = {\n",
    "    \"data\": '../input/bird-dataset-cropped/bird_dataset',\n",
    "    \"batch_size\": 10,\n",
    "    \"epochs\": 150,\n",
    "    \"lr\": 1e-2,\n",
    "    \"momentum\": 0.9,\n",
    "    \"seed\": 9823,\n",
    "    \"weight_decay\" : 3e-5,\n",
    "    \"experiment\": 'experiment',\n",
    "    \"checkpoint\": None,\n",
    "    \"clipping\": 1.\n",
    "}\n",
    "\n",
    "# torch.manual_seed(config[\"seed\"])\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create experiment folder\n",
    "if not os.path.isdir(config[\"experiment\"]):\n",
    "    os.makedirs(config[\"experiment\"])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(config[\"data\"] + '/train_images',transform=None)\n",
    "val_dataset = datasets.ImageFolder(config[\"data\"] + '/val_images',transform=data_transforms)\n",
    "\n",
    "train_dataset = AugmentedDS(train_dataset,transform=train_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=1,\n",
    "    pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False, \n",
    "    num_workers=1,\n",
    "    pin_memory=True)\n",
    "\n",
    "visualize_augmentations(train_dataset,20)\n",
    "\n",
    "# Loss function + weighted classes\n",
    "weights = torch.ones(nclasses)\n",
    "weights[16] = 0.7\n",
    "weights[13] = 0.8\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights.to(device),reduction='mean')\n",
    "\n",
    "# Neural network and optimizer\n",
    "model = BEiTNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"], weight_decay=config[\"weight_decay\"])\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,150)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,[10,20,30],0.5)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "if config[\"checkpoint\"] is not None:\n",
    "    checkpoint = torch.load(config[\"checkpoint\"],map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    del checkpoint\n",
    "\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T19:20:49.445239Z",
     "iopub.status.busy": "2021-11-22T19:20:49.444985Z"
    },
    "id": "NNgb-NMdnnfQ",
    "outputId": "7db42fc9-a4bc-412e-d298-6105ad5f88e6"
   },
   "outputs": [],
   "source": [
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            return img.convert('RGB')\n",
    "\n",
    "def eval(outfile):\n",
    "    test_dir = config[\"data\"] + '/test_images/mistery_category'\n",
    "    output_file = open(outfile, \"w\")\n",
    "    output_file.write(\"Id,Category\\n\")\n",
    "    for f in os.listdir(test_dir):\n",
    "        if 'jpg' in f:\n",
    "            data = data_transforms(pil_loader(test_dir + '/' + f))\n",
    "            data = data.view(1, data.size(0), data.size(1), data.size(2)).to(device)\n",
    "            output = model(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config[\"clipping\"])\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        del data\n",
    "        del target\n",
    "    # scheduler.step()\n",
    "    return epoch_loss/len(train_loader)\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    validation_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "      for data, target in val_loader:\n",
    "          data = data.to(device)\n",
    "          target = target.to(device)\n",
    "          output = model(data)\n",
    "          # sum up batch loss\n",
    "          validation_loss += criterion(output, target).item()\n",
    "          # get the index of the max log-probability\n",
    "          pred = output.max(1, keepdim=True)[1]\n",
    "          correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "          del data\n",
    "          del target\n",
    "\n",
    "    validation_loss /= len(val_loader.dataset)\n",
    "    accuracy = 100. * correct / len(val_loader.dataset)\n",
    "    return validation_loss, accuracy\n",
    "\n",
    "run = wandb.init(\n",
    "  project=\"recvis-tp3\",\n",
    "  config=config\n",
    ")\n",
    "\n",
    "with tqdm(range(config[\"epochs\"])) as t:\n",
    "    acc_best = 0\n",
    "    loss_best = 100\n",
    "    for epoch in t:\n",
    "        loss_train = train(epoch) # Train\n",
    "        loss_val, acc_val = validation() # Test\n",
    "        scheduler.step()\n",
    "        # Save current checkpoint\n",
    "        if (acc_val > 90):\n",
    "            eval(f'experiment/model_{epoch}_{int(100.*acc_val)/100.}.csv')\n",
    "        if ((acc_val > acc_best) or ((acc_val == acc_best) and (loss_val < loss_best))) and (acc_val > 90):\n",
    "            torch.save(model.state_dict(), f'experiment/best.pth')\n",
    "            acc_best = acc_val\n",
    "            loss_best = loss_val\n",
    "        t.set_postfix(loss_train=loss_train,loss_val=loss_val,acc_val=acc_val)\n",
    "        wandb.log({\n",
    "            \"loss_train\" : loss_train,\n",
    "            \"loss_test\" : loss_val,\n",
    "            \"acc_test\" : acc_val,\n",
    "            \"lr\": scheduler.get_last_lr()[0]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
